# AI_project_master/app.py
import streamlit as st
import pandas as pd
import joblib, json, time, traceback
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score

st.set_page_config(page_title="Earthquake Alert", page_icon="üåé", layout="wide")

# --- Robust paths: based on this file location ---
BASE = Path(__file__).resolve().parent
DATA_PATH   = BASE / "data" / "earthquakes.csv"
MODELS_DIR  = BASE / "models"
MODEL_PATH  = MODELS_DIR / "earthquake_model.pkl"
ENC_PATH    = MODELS_DIR / "label_encoder.pkl"
STORE_DIR   = BASE / "storage"
ANN_PATH    = STORE_DIR / "public_announcements.json"
METRICS_DIR = BASE / "metrics"
METRICS_CSV = METRICS_DIR / "metrics.csv"

st.title("üåé ‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÅ‡∏ú‡πà‡∏ô‡∏î‡∏¥‡∏ô‡πÑ‡∏´‡∏ß")
st.caption("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå/‡∏Å‡∏£‡∏≠‡∏Å‡∏Ñ‡πà‡∏≤ ‚Üí ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ AI (Decision Tree) ‚Üí ‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®  ‚Ä¢  ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏∏‡∏î‡∏ï‡∏≤‡∏°‡πÇ‡∏à‡∏ó‡∏¢‡πå")

# ---------- Helper ----------
def safe_read_csv(p: Path):
    try:
        return pd.read_csv(p)
    except Exception as e:
        st.error(f"‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {p}\n\n{e}")
        st.code(traceback.format_exc())
        return None

def quick_train_decision_tree(df: pd.DataFrame):
    """‡πÄ‡∏ó‡∏£‡∏ô Decision Tree ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÉ‡∏´‡πâ‡∏ú‡πà‡∏≤‡∏ô‡πÇ‡∏à‡∏ó‡∏¢‡πå (‡∏Ç‡πâ‡∏≠ 3‚Äì4) + ‡∏™‡∏£‡πâ‡∏≤‡∏á metrics.csv"""
    num_cols_all = ["magnitude", "depth", "cdi", "mmi", "sig"]
    have_cols = [c for c in num_cols_all if c in df.columns]
    if "alert" not in df.columns:
        st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'alert' ‡πÉ‡∏ô dataset ‚Äî ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢")
        return False

    # numeric cleanup
    for c in have_cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
        df[c].fillna(df[c].median(), inplace=True)

    le = LabelEncoder()
    y = le.fit_transform(df["alert"].astype(str))

    feature_sets = {
        "F1_basic": [c for c in ["magnitude","depth","sig"] if c in have_cols],
        "F2_all"  : [c for c in ["magnitude","depth","cdi","mmi","sig"] if c in have_cols],
    }
    param_grid = [
        {"max_depth": d, "min_samples_split": mss}
        for d in [None, 5, 10, 20]
        for mss in [2, 5, 10]
    ]

    rows = []
    best = {"acc": -1, "model": None, "features": None, "params": None}

    for fname, feats in feature_sets.items():
        if not feats:
            continue
        X = df[feats]
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        for params in param_grid:
            clf = DecisionTreeClassifier(random_state=42, **params)
            clf.fit(X_train, y_train)
            acc = accuracy_score(y_test, clf.predict(X_test))
            rows.append({
                "feature_set": fname,
                "features": ",".join(feats),
                "params": json.dumps(params),
                "accuracy": round(acc, 4),
            })
            if acc > best["acc"]:
                best = {"acc": acc, "model": clf, "features": feats, "params": params}

    # save outputs
    METRICS_DIR.mkdir(parents=True, exist_ok=True)
    pd.DataFrame(rows).sort_values("accuracy", ascending=False).to_csv(METRICS_CSV, index=False)
    MODELS_DIR.mkdir(parents=True, exist_ok=True)
    joblib.dump(best["model"], MODEL_PATH)
    joblib.dump(le, ENC_PATH)
    st.success(f"‡πÄ‡∏ó‡∏£‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! Best accuracy={round(best['acc'],4)} | "
               f"features={best['features']} | params={best['params']}")
    return True

# ---------- Section: Diagnostics (‡πÑ‡∏°‡πà‡∏´‡∏¢‡∏∏‡∏î‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö) ----------
with st.expander("üß∞ ‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (Diagnostics)"):
    st.write("‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:")
    st.code(f"""
BASE        = {BASE}
DATA_PATH   = {DATA_PATH}
MODEL_PATH  = {MODEL_PATH}
ENC_PATH    = {ENC_PATH}
METRICS_CSV = {METRICS_CSV}
ANN_PATH    = {ANN_PATH}
""", language="text")

    exists = {
        "earthquakes.csv": DATA_PATH.exists(),
        "earthquake_model.pkl": MODEL_PATH.exists(),
        "label_encoder.pkl": ENC_PATH.exists(),
        "metrics.csv": METRICS_CSV.exists(),
    }
    st.write("‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:")
    st.json(exists)

# ---------- Load dataset (‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏à‡πâ‡∏á‡∏õ‡∏±‡∏ç‡∏´‡∏≤) ----------
df = None
if not DATA_PATH.exists():
    st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå dataset: data/earthquakes.csv")
    st.info("‡πÇ‡∏õ‡∏£‡∏î‡∏ß‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Kaggle ‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà: AI_project_master/data/earthquakes.csv ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏î Refresh")
else:
    df = safe_read_csv(DATA_PATH)

# ---------- Quick Train button (‡πÅ‡∏Å‡πâ‡πÄ‡∏Ñ‡∏™‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•/metrics) ----------
if df is not None and (not MODEL_PATH.exists() or not ENC_PATH.exists() or not METRICS_CSV.exists()):
    st.warning("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•/encoder ‡∏´‡∏£‡∏∑‡∏≠ metrics.csv ‚Äî ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ó‡∏£‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ")
    if st.button("üöÄ Quick Train (Decision Tree) ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡πÇ‡∏à‡∏ó‡∏¢‡πå 3‚Äì4"):
        ok = quick_train_decision_tree(df)
        if ok:
            st.experimental_rerun()

# ---------- ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô (‡∏Ç‡πâ‡∏≠ 4) ----------
with st.expander("üìä Training Results (‡∏´‡∏•‡∏≤‡∏¢ features/parameters)"):
    if METRICS_CSV.exists():
        try:
            mdf = pd.read_csv(METRICS_CSV)
            if not mdf.empty:
                st.dataframe(mdf, use_container_width=True)
                top = mdf.sort_values("accuracy", ascending=False).iloc[0]
                st.success(
                    f"‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å: feature_set = {top['feature_set']}, "
                    f"params = {top['params']}, accuracy = {top['accuracy']}"
                )
            else:
                st.info("metrics.csv ‡∏ß‡πà‡∏≤‡∏á ‚Äî ‡∏Å‡∏î Quick Train ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏±‡∏ô train.py")
        except Exception as e:
            st.error(f"‡∏≠‡πà‡∏≤‡∏ô metrics.csv ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
    else:
        st.info("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏ö metrics.csv ‚Äî ‡∏Å‡∏î Quick Train ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏±‡∏ô train.py")

# ---------- ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ----------
model, le = None, None
if MODEL_PATH.exists() and ENC_PATH.exists():
    try:
        model = joblib.load(MODEL_PATH)
        le    = joblib.load(ENC_PATH)
    except Exception as e:
        st.error(f"‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•/encoder ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
        st.code(traceback.format_exc())

# ---------- UI ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏•‡∏∑‡∏≠‡∏Å/‡∏Å‡∏£‡∏≠‡∏Å + ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ----------
if df is not None and model is not None and le is not None:
    # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö UI
    for col in ["magnitude","depth","cdi","mmi","sig"]:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")
            df[col].fillna(df[col].median(), inplace=True)

    st.subheader("üëÆ‚Äç‚ôÄÔ∏è ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏£‡∏≠‡∏Å‡∏Ñ‡πà‡∏≤‡πÄ‡∏≠‡∏á ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏î‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢")
    latest = df.tail(200).reset_index(drop=True)

    left, right = st.columns([1,1])
    with left:
        st.markdown("**‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå (200 ‡πÅ‡∏ñ‡∏ß‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)**")
        idx = st.number_input("‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡πÅ‡∏ñ‡∏ß:", min_value=0, max_value=len(latest)-1,
                              value=len(latest)-1, step=1)
        row = latest.iloc[int(idx)].to_dict()

    with right:
        st.markdown("**‡∏Å‡∏£‡∏≠‡∏Å‡∏Ñ‡πà‡∏≤‡πÄ‡∏≠‡∏á (‡∏ó‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏î‡πâ)**")
        def defval(k, default):
            v = row.get(k, default)
            try:
                return float(v) if pd.notna(v) else float(default)
            except Exception:
                return float(default)
        mag = st.number_input("magnitude", value=defval("magnitude", 5.0))
        dep = st.number_input("depth",     value=defval("depth", 10.0))
        cdi = st.number_input("cdi",       value=defval("cdi", 3.0))
        mmi = st.number_input("mmi",       value=defval("mmi", 3.0))
        sig = st.number_input("sig",       value=defval("sig", 300.0))

        inputs = pd.DataFrame([{
            "magnitude": mag, "depth": dep, "cdi": cdi, "mmi": mmi, "sig": sig
        }])

    if st.button("üß† ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ AI", use_container_width=True):
        feat_cols = [c for c in ["magnitude","depth","cdi","mmi","sig"] if c in df.columns]
        X = inputs[feat_cols]
        y_id = model.predict(X)[0]
        y_label = le.inverse_transform([y_id])[0]
        st.success(f"‡∏ú‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô: **{str(y_label).upper()}**")
        st.session_state["last_pred"] = {
            "risk": str(y_label),
            "inputs": inputs.iloc[0].to_dict(),
            "region": str(row.get("place", "Affected area")) if "place" in row else "Affected area"
        }

    # ‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®
    if "last_pred" in st.session_state:
        st.divider()
        st.subheader("üì¢ ‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® (‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á)")
        pred = st.session_state["last_pred"]
        region = st.text_input("‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà/‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ", value=pred["region"])
        msg_default = f"‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏±‡πà‡∏ô‡∏™‡∏∞‡πÄ‡∏ó‡∏∑‡∏≠‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö {pred['risk'].upper()} ‚Äî ‡πÇ‡∏õ‡∏£‡∏î‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢"
        message = st.text_area("‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®", value=msg_default, height=80)
        tips = ["‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ï‡πâ‡πÇ‡∏ï‡πä‡∏∞/‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏£‡∏á", "‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏•‡∏¥‡∏ü‡∏ï‡πå/‡∏Å‡∏£‡∏∞‡∏à‡∏Å", "‡∏õ‡∏¥‡∏î‡πÅ‡∏Å‡πä‡∏™/‡πÑ‡∏ü‡∏ü‡πâ‡∏≤"]

        if st.button("‚úÖ ‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®", type="primary", use_container_width=True):
            STORE_DIR.mkdir(parents=True, exist_ok=True)
            doc = {
                "last_updated": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
                "announcements": [{
                    "id": str(int(time.time())),
                    "region": region,
                    "risk_level": pred["risk"],
                    "message": message,
                    "tips": tips,
                    "inputs": pred["inputs"]
                }]
            }
            with open(ANN_PATH, "w", encoding="utf-8") as f:
                json.dump(doc, f, ensure_ascii=False, indent=2)
            st.success("‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡πÅ‡∏•‡πâ‡∏ß! ‡πÄ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏•‡∏á‡πÑ‡∏õ‡∏î‡∏π‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ üëá")

# ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‡πÄ‡∏´‡πá‡∏ô
st.divider()
st.subheader("üö® ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® (‡πÅ‡∏™‡∏î‡∏á‡∏ï‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô)")
if ANN_PATH.exists():
    ann = json.load(open(ANN_PATH, encoding="utf-8"))
    st.caption(f"‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î: {ann.get('last_updated','-')}")
    for a in ann.get("announcements", []):
        level = str(a.get("risk_level","")).lower()
        color = {"green":"üü¢","yellow":"üü°","orange":"üü†","red":"üî¥"}.get(level, "üî∂")
        st.markdown(f"### {color} ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô: **{str(a.get('risk_level','')).upper()}**")
        st.write(f"‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà: **{a.get('region','-')}**")
        if a.get("message"): st.write(a["message"])
        if a.get("tips"):
            st.write("‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:")
            for t in a["tips"]:
                st.write(f"- {t}")
else:
    st.info("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î")

# Credits (‡∏ï‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠ 2)
with st.expander("‚ÑπÔ∏è Dataset Source / ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á"):
    st.write("Dataset: Kaggle ‚Äì Earthquake Alert Prediction (Ahmed Uzaki) ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (‡πÇ‡∏õ‡∏£‡∏î‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤‡πÉ‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô)")
